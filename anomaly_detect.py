import numpy as np
import pandas as pd
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.seasonal import STL
import joblib
import warnings
import json
from datetime import datetime
from collections import deque
warnings.filterwarnings('ignore')

# ======================
# å¢å¼ºå‹å‘¨æœŸè®¡ç®—å‡½æ•°
# ======================
def calculate_period(sampling_interval, physical_cycle):
    """æ ¹æ®ç‰©ç†å‘¨æœŸè‡ªåŠ¨è®¡ç®—STLå‚æ•°"""
    # ç¡®ä¿æœ€å°æ•°æ®é‡çº¦æŸ
    min_points = 2 * int(physical_cycle / sampling_interval)  
    
    # å‘¨æœŸç‚¹è®¡ç®—ï¼ˆä¿ç•™æ•´æ•°ï¼‰
    period_points = round(physical_cycle / sampling_interval)
    
    # å·¥ä¸šçº§æ ¡éªŒ
    if period_points < 4:
        raise ValueError(f"é‡‡æ ·ç‡ä¸è¶³ï¼ç‰©ç†å‘¨æœŸ{physical_cycle}séœ€è¦"
                         f">={4*sampling_interval}Hzé‡‡æ ·ç‡")
    
    # ç¡®ä¿å‘¨æœŸä¸ºå¥‡æ•°ï¼ˆSTLå†…éƒ¨è¦æ±‚ï¼‰
    return int(period_points) if int(period_points) % 2 == 1 else int(period_points) + 1

# ======================
# æ—¶åºç‰¹å¾å·¥ç¨‹æ¨¡å—ï¼ˆå·¥ä¸šå¢å¼ºç‰ˆï¼‰
# ======================
class TimeSeriesFeatureEngineer:
    def __init__(self, physical_cycle=None, sampling_interval=None):
        """
        åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å‚æ•°
        ç‰©ç†å‘¨æœŸï¼ˆç§’ï¼‰å’Œé‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰å…±åŒç¡®å®šå‘¨æœŸå‚æ•°
        """
        # åŠ¨æ€è®¡ç®—å‘¨æœŸç‚¹
        if physical_cycle and sampling_interval:
            self.period = calculate_period(sampling_interval, physical_cycle)
            print(f"âœ… è‡ªåŠ¨è®¡ç®—STLå‘¨æœŸ: {self.period}ç‚¹ (ç‰©ç†å‘¨æœŸ={physical_cycle}s, é‡‡æ ·é—´éš”={sampling_interval}s)")
        else:
            self.period = 1440  # é»˜è®¤æ—¥å‘¨æœŸï¼ˆæ¯åˆ†é’Ÿé‡‡æ ·ï¼‰
            print("âš ï¸ ä½¿ç”¨é»˜è®¤å‘¨æœŸ1440ç‚¹ï¼ˆæ—¥å‘¨æœŸæ¯åˆ†é’Ÿé‡‡æ ·ï¼‰")
        
        # æ ‡å‡†åŒ–å·¥å…·
        self.scaler = StandardScaler()
        # çª—å£å¤§å°å†å²è®°å½•ï¼ˆå¹³æ»‘ç”¨ï¼‰
        self.window_history = deque(maxlen=5)
        
    def _stl_decomposition(self, values):
        """é²æ£’STLæ—¶åºåˆ†è§£è·å–æ®‹å·®"""
        try:
            # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦è¶³å¤Ÿ
            if len(values) < 2 * self.period:
                raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³{2*self.period}ç‚¹ï¼Œæ— æ³•è¿›è¡ŒSTLåˆ†è§£")
                
            stl = STL(values, period=self.period, robust=True)
            res = stl.fit()
            return res.resid
        except Exception as e:
            print(f"âš ï¸ STLåˆ†è§£å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é›¶æ®‹å·®æ›¿ä»£")
            return np.zeros_like(values)
    
    def _safe_rolling(self, series, window, func):
        """å¸¦å¼‚å¸¸å¤„ç†çš„æ»‘åŠ¨çª—å£è®¡ç®—"""
        try:
            # çª—å£å¤§å°å¹³æ»‘
            self.window_history.append(window)
            smoothed_window = int(np.mean(self.window_history))
            
            # æ£€æŸ¥çª—å£å†…NaNæ¯”ä¾‹
            if series.isna().sum() > 0.3 * smoothed_window:
                return series.rolling(smoothed_window, min_periods=1).median()
            else:
                return func(series, smoothed_window)
        except Exception as e:
            print(f"âš ï¸ æ»‘åŠ¨çª—å£è®¡ç®—å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨ä¸­ä½æ•°æ›¿ä»£")
            return series.rolling(5, min_periods=1).median()

    def create_features(self, df):
        """
        è¾“å…¥: DataFrameåŒ…å«timestampå’Œvalueåˆ—
        è¾“å‡º: å¢å¼ºç‰¹å¾çŸ©é˜µï¼ˆå·²å¤„ç†NaNï¼‰
        """
        # å…‹éš†æ•°æ®é¿å…æ±¡æŸ“åŸå§‹æ•°æ®
        df = df.copy()
        
        # åŸºç¡€ç‰¹å¾
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        
        # æ»åç‰¹å¾ï¼ˆå·¥ä¸šä¼˜åŒ–ï¼šå‰å‘å¡«å……å¤„ç†åˆå§‹NaNï¼‰
        df['lag_1'] = df['value'].shift(1).fillna(method='bfill')
        df['lag_24'] = df['value'].shift(self.period).fillna(method='bfill')
        
        # è‡ªé€‚åº”çª—å£å¤§å°
        window_size = max(6, min(60, len(df)//10))  # é™åˆ¶åœ¨6-60ä¹‹é—´
        
        # å®‰å…¨æ»‘åŠ¨çª—å£è®¡ç®—
        df['rolling_mean'] = self._safe_rolling(df['value'], window_size, 
                                               lambda s, w: s.rolling(w, min_periods=1).mean())
        df['rolling_std'] = self._safe_rolling(df['value'], window_size, 
                                              lambda s, w: s.rolling(w, min_periods=1).std())
        
        # STLæ®‹å·®ç‰¹å¾
        df['residual'] = self._stl_decomposition(df['value'].values)
        
        # å‘¨æœŸç¼–ç 
        df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)
        df['week_sin'] = np.sin(2*np.pi*df['day_of_week']/7)
        df['week_cos'] = np.cos(2*np.pi*df['day_of_week']/7)
        
        # ç‰¹å¾é€‰æ‹©
        features = df[['value', 'lag_1', 'lag_24', 'rolling_mean', 
                      'rolling_std', 'residual', 'hour_sin', 'hour_cos',
                      'week_sin', 'week_cos']]
        
        # ==== å·¥ä¸šçº§NaNå¤„ç† ====
        # 1. æ›¿æ¢æ— ç©·å€¼
        features = features.replace([np.inf, -np.inf], np.nan)
        
        # 2. è®°å½•åˆå§‹NaNåˆ†å¸ƒ
        initial_nan = features.isna().sum().sum()
        
        # 3. åˆ†ç±»å‹å¡«å……ç­–ç•¥
        for col in features.columns:
            col_data = features[col]
            nan_count = col_data.isna().sum()
            
            if nan_count == 0:
                continue
                
            # æ ¹æ®ç‰¹å¾ç±»å‹é€‰æ‹©å¡«å……ç­–ç•¥
            if 'lag' in col:
                features[col] = col_data.fillna(method='bfill')
            elif 'rolling' in col:
                features[col] = col_data.interpolate(method='linear')
            elif 'residual' in col:
                features[col] = col_data.fillna(0)  # æ®‹å·®ç¼ºå¤±è§†ä¸ºæ­£å¸¸
            else:
                features[col] = col_data.fillna(col_data.median())
        
        # 4. æœ€ç»ˆæ£€æŸ¥
        final_nan = features.isna().sum().sum()
        if final_nan > 0:
            print(f"âš ï¸ è­¦å‘Šï¼šæ®‹ç•™ {final_nan} ä¸ªNaNï¼Œä½¿ç”¨å…¨å±€ä¸­ä½æ•°å¡«å……")
            features = features.fillna(features.median())
        
        # 5. æŠ¥å‘Šæ•°æ®è´¨é‡
        if initial_nan > 0:
            print(f"ğŸ›  å¤„ç†NaNï¼šåˆå§‹{initial_nan} â†’ å¤„ç†å{final_nan}")
        
        # æ ‡å‡†åŒ–ï¼ˆä»…ç”¨æœ‰æ•ˆæ•°æ®è®­ç»ƒï¼‰
        if not hasattr(self, 'fitted_scaler'):
            valid_features = features.dropna()
            if len(valid_features) < 100:
                print("âŒ æœ‰æ•ˆæ•°æ®ä¸è¶³100ç‚¹ï¼Œæ— æ³•è®­ç»ƒæ ‡å‡†åŒ–å™¨")
                return None
            self.fitted_scaler = self.scaler.fit(valid_features)
        
        return self.fitted_scaler.transform(features)

# ======================
# åŠ¨æ€OCSVMæ¨¡å‹ï¼ˆå·¥ä¸šå¢å¼ºç‰ˆï¼‰
# ======================
class DynamicOCSVM:
    def __init__(self, nu=0.03, gamma='scale', update_interval=24):
        """
        nu: é¢„æœŸå¼‚å¸¸æ¯”ä¾‹
        gamma: RBFæ ¸å‚æ•°
        update_interval: æ¨¡å‹æ›´æ–°é—´éš”(å°æ—¶)
        """
        self.nu = nu
        self.gamma = gamma
        self.update_interval = update_interval
        self.last_update_time = None
        self.model = OneClassSVM(nu=nu, kernel='rbf', gamma=gamma)
        # æ”¯æŒå‘é‡ç¼“å­˜
        self.support_vectors = None
        
    def needs_update(self, current_time):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ¨¡å‹"""
        if self.last_update_time is None:
            return True
        hours_passed = (current_time - self.last_update_time).total_seconds()/3600
        return hours_passed >= self.update_interval
    
    def _nan_safety_check(self, X):
        """å·¥ä¸šçº§NaNæ£€æŸ¥ä¸å¤„ç†"""
        if X is None:
            raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
            
        nan_mask = np.isnan(X)
        nan_count = np.sum(nan_mask)
        
        if nan_count > 0:
            nan_percentage = 100 * nan_count / X.size
            print(f"âš ï¸ æ¨¡å‹è¾“å…¥åŒ…å« {nan_count} ä¸ªNaN ({nan_percentage:.2f}%)")
            
            # ç®€å•å¡«å……ç­–ç•¥
            col_medians = np.nanmedian(X, axis=0)
            for i in range(X.shape[1]):
                X[np.isnan(X[:, i]), i] = col_medians[i]
                
        return X
    
    def train(self, X):
        """é²æ£’è®­ç»ƒæ–¹æ³•"""
        # æ•°æ®è´¨é‡æ£€æŸ¥
        X = self._nan_safety_check(X)
        
        print(f"â³ åˆå§‹æ¨¡å‹è®­ç»ƒä¸­ï¼Œæ ·æœ¬æ•°: {len(X)}")
        try:
            self.model.fit(X)
            self.last_update_time = pd.Timestamp.now()
            self.support_vectors = self.model.support_vectors_.copy()
            print(f"âœ… è®­ç»ƒæˆåŠŸï¼Œæ”¯æŒå‘é‡æ•°: {len(self.support_vectors)}")
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
            # åº”æ€¥æ–¹æ¡ˆï¼šåˆ›å»ºè™šæ‹Ÿæ¨¡å‹
            self.model = OneClassSVM(nu=self.nu, kernel='rbf', gamma=self.gamma)
            dummy_data = np.random.randn(100, X.shape[1])
            self.model.fit(dummy_data)
            self.support_vectors = dummy_data
            print("âš ï¸ å·²åŠ è½½åº”æ€¥æ¨¡å‹")
        
        return self
    
    def partial_update(self, X_new):
        """å¢é‡æ›´æ–°æ¨¡å‹"""
        if len(X_new) < 50:  # é™ä½æ ·æœ¬è¦æ±‚
            print(f"âš ï¸ æ–°å¢æ ·æœ¬ä¸è¶³({len(X_new)}<50)ï¼Œè·³è¿‡æ›´æ–°")
            return
            
        print(f"ğŸ”„ æ¨¡å‹å¢é‡æ›´æ–°ä¸­ï¼Œæ–°å¢æ ·æœ¬: {len(X_new)}")
        
        try:
            # åˆå¹¶æ–°æ—§æ•°æ®ï¼ˆä¿ç•™å†å²æ”¯æŒå‘é‡ï¼‰
            X_combined = np.vstack([self.support_vectors, X_new])
            
            # åˆ›å»ºæ–°æ¨¡å‹
            new_model = OneClassSVM(nu=self.nu, kernel='rbf', gamma=self.gamma)
            new_model.fit(X_combined)
            
            # æ›´æ–°çŠ¶æ€
            self.model = new_model
            self.support_vectors = new_model.support_vectors_.copy()
            self.last_update_time = pd.Timestamp.now()
            print(f"âœ… æ›´æ–°å®Œæˆï¼Œæ”¯æŒå‘é‡æ•°: {len(self.support_vectors)}")
        except Exception as e:
            print(f"âŒ å¢é‡æ›´æ–°å¤±è´¥: {str(e)}ï¼Œä¿ç•™æ—§æ¨¡å‹")

    def predict(self, X, threshold=0.0):
        """é¢„æµ‹å¼‚å¸¸åˆ†æ•°å’Œæ ‡ç­¾"""
        # æ•°æ®è´¨é‡æ£€æŸ¥
        X = self._nan_safety_check(X)
        
        try:
            scores = -self.model.decision_function(X)
            return scores, scores > threshold
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {str(e)}ï¼Œè¿”å›é›¶åˆ†æ•°")
            return np.zeros(len(X)), np.zeros(len(X), dtype=bool)

# ======================
# å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿä¸»æµç¨‹ï¼ˆå·¥ä¸šçº§ï¼‰
# ======================
class IndustrialAnomalyDetector:
    def __init__(self, physical_cycle=86400, sampling_interval=300):
        """
        physical_cycle: ç‰©ç†å‘¨æœŸï¼ˆç§’ï¼‰ï¼Œå¦‚86400=24å°æ—¶
        sampling_interval: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰ï¼Œå¦‚300=5åˆ†é’Ÿ
        """
        self.feature_engineer = TimeSeriesFeatureEngineer(
            physical_cycle=physical_cycle,
            sampling_interval=sampling_interval
        )
        self.detector = DynamicOCSVM()
        self.data_buffer = pd.DataFrame(columns=['timestamp', 'value'])
        self.threshold_history = []
        # æ•°æ®è´¨é‡ç›‘æ§
        self.data_quality_log = []
        
    def _log_data_quality(self, nan_count, total_points):
        """è®°å½•æ•°æ®è´¨é‡é—®é¢˜"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "nan_count": nan_count,
            "total_points": total_points,
            "nan_percentage": 100 * nan_count / total_points
        }
        self.data_quality_log.append(entry)
        
        # ä¸¥é‡é—®é¢˜å‘Šè­¦
        if entry["nan_percentage"] > 20:
            print(f"ğŸš¨ ä¸¥é‡æ•°æ®è´¨é‡é—®é¢˜ï¼NaNæ¯”ä¾‹: {entry['nan_percentage']:.1f}%")
    
    def process_stream(self, new_data):
        """
        å¤„ç†å®æ—¶æ•°æ®æµ
        è¾“å…¥: DataFrame åŒ…å« ['timestamp', 'value']
        """
        # 0. è¾“å…¥éªŒè¯
        if new_data.isnull().any().any():
            nan_count = new_data.isnull().sum().sum()
            self._log_data_quality(nan_count, new_data.size)
        
        # 1. æ•°æ®ç¼“å†²ï¼ˆå¸¦æ—¶é—´æ’åºï¼‰
        self.data_buffer = pd.concat([self.data_buffer, new_data])
        self.data_buffer.sort_values('timestamp', inplace=True)
        self.data_buffer.drop_duplicates('timestamp', inplace=True)
        
        # 2. ç‰¹å¾å·¥ç¨‹ï¼ˆå·²å†…ç½®NaNå¤„ç†ï¼‰
        current_time = new_data['timestamp'].max()
        features = self.feature_engineer.create_features(self.data_buffer.copy())
        
        # ç‰¹å¾å·¥ç¨‹å¤±è´¥å¤„ç†
        if features is None:
            print("âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ")
            return np.zeros(len(new_data)), np.zeros(len(new_data), dtype=bool)
        
        # 3. æ¨¡å‹åˆå§‹è®­ç»ƒï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
        if not hasattr(self.detector.model, 'fit_status_'):
            min_data_size = max(100, 2 * self.feature_engineer.period)
            if len(self.data_buffer) < min_data_size:
                print(f"âŒ› ç­‰å¾…æ•°æ®ç´¯ç§¯ ({len(self.data_buffer)}/{min_data_size})")
                return np.zeros(len(new_data)), np.zeros(len(new_data), dtype=bool)
            
            print(f"ğŸš€ å¼€å§‹åˆå§‹è®­ç»ƒï¼Œæ•°æ®é‡: {len(features)}")
            self.detector.train(features)
        
        # 4. åŠ¨æ€æ¨¡å‹æ›´æ–°
        if self.detector.needs_update(current_time):
            recent_data = self.data_buffer[self.data_buffer['timestamp'] >= self.detector.last_update_time]
            if len(recent_data) > 0:
                print(f"â± è§¦å‘æ¨¡å‹æ›´æ–°ï¼Œæ–°æ•°æ®é‡: {len(recent_data)}")
                recent_features = self.feature_engineer.create_features(recent_data)
                if recent_features is not None:
                    self.detector.partial_update(recent_features)
        
        # 5. å®æ—¶é¢„æµ‹ï¼ˆä»…å¯¹æ–°æ•°æ®ï¼‰
        new_indices = self.data_buffer.index[-len(new_data):]
        new_features = features[new_indices]
        scores, labels = self.detector.predict(new_features)
        
        # 6. åŠ¨æ€é˜ˆå€¼è°ƒæ•´
        try:
            current_threshold = self._calculate_dynamic_threshold(scores)
            refined_labels = scores > current_threshold
        except Exception as e:
            print(f"âŒ é˜ˆå€¼è®¡ç®—å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼")
            refined_labels = scores > np.percentile(scores, 95)
        
        return scores, refined_labels
    
    def _calculate_dynamic_threshold(self, scores):
        """åŸºäºå†å²æ•°æ®çš„åŠ¨æ€é˜ˆå€¼è®¡ç®—"""
        # è·å–å½“å‰å°æ—¶
        current_hour = pd.Timestamp.now().hour
        
        # æ›´æ–°é˜ˆå€¼å†å²è®°å½•
        hour_scores = {'hour': current_hour, 'scores': scores}
        self.threshold_history.append(hour_scores)
        
        # ä¿ç•™æœ€è¿‘7å¤©æ•°æ®
        if len(self.threshold_history) > 168:  # 24*7
            self.threshold_history.pop(0)
        
        # è®¡ç®—å½“å‰æ—¶æ®µçš„ç»Ÿè®¡é‡
        hour_data = [s for h in self.threshold_history 
                    if h['hour'] == current_hour for s in h['scores']]
        
        if len(hour_data) > 20:  # æé«˜æœ€å°æ ·æœ¬è¦æ±‚
            q75 = np.percentile(hour_data, 75)
            q25 = np.percentile(hour_data, 25)
            iqr = q75 - q25
            
            # IQRä¿æŠ¤æœºåˆ¶
            iqr = max(iqr, 0.01 * (np.max(hour_data) - np.min(hour_data)))
            
            threshold = q75 + 1.5 * iqr
            print(f"ğŸ“Š åŠ¨æ€é˜ˆå€¼: {threshold:.4f} (Q75={q75:.4f}, IQR={iqr:.4f})")
        else:
            threshold = np.percentile(scores, 95)
            print(f"ğŸ“Š åˆå§‹é˜ˆå€¼: {threshold:.4f} (P95)")
        
        return threshold

    def save_state(self, file_path):
        """ä¿å­˜ç³»ç»ŸçŠ¶æ€"""
        state = {
            'data_buffer': self.data_buffer,
            'threshold_history': self.threshold_history,
            'data_quality_log': self.data_quality_log,
            'feature_engineer': self.feature_engineer,
            'detector': self.detector
        }
        joblib.dump(state, file_path)
        print(f"ğŸ’¾ ç³»ç»ŸçŠ¶æ€å·²ä¿å­˜è‡³ {file_path}")
    
    def load_state(self, file_path):
        """åŠ è½½ç³»ç»ŸçŠ¶æ€"""
        state = joblib.load(file_path)
        self.data_buffer = state['data_buffer']
        self.threshold_history = state['threshold_history']
        self.data_quality_log = state['data_quality_log']
        self.feature_engineer = state['feature_engineer']
        self.detector = state['detector']
        print(f"ğŸ” ç³»ç»ŸçŠ¶æ€å·²ä» {file_path} åŠ è½½")

# ======================
# ä½¿ç”¨ç¤ºä¾‹ï¼ˆå·¥ä¸šçº§æµ‹è¯•ï¼‰
# ======================
if __name__ == "__main__":
    # 1. æ¨¡æ‹Ÿæ—¶åºæ•°æ®ç”Ÿæˆï¼ˆå¸¦NaNæ¨¡æ‹Ÿï¼‰
    def generate_industrial_data(num_points=5000, anomaly_rate=0.01, nan_rate=0.03):
        base_time = pd.Timestamp.now() - pd.Timedelta(days=7)
        timestamps = pd.date_range(start=base_time, periods=num_points, freq='5min')
        
        # åŸºç¡€ä¿¡å·ï¼šè¶‹åŠ¿+å‘¨æœŸ
        trend = np.linspace(0, 5, num_points)
        seasonal = 3 * np.sin(2 * np.pi * np.arange(num_points)/288)  # æ—¥å‘¨æœŸ(24h/5min=288)
        
        # æ·»åŠ å™ªå£°
        values = trend + seasonal + np.random.normal(0, 0.8, num_points)
        
        # æ’å…¥äººå·¥å¼‚å¸¸
        anomaly_count = int(num_points * anomaly_rate)
        anomaly_points = np.random.choice(num_points, size=anomaly_count, replace=False)
        values[anomaly_points] += np.random.uniform(4, 10, anomaly_count)
        
        # æ’å…¥NaNå€¼
        nan_points = np.random.choice(num_points, size=int(num_points * nan_rate), replace=False)
        values[nan_points] = np.nan
        
        df = pd.DataFrame({'timestamp': timestamps, 'value': values})
        
        # å‰å‘å¡«å……NaN
        df['value'] = df['value'].fillna(method='ffill')
        return df
    
    # 2. åˆå§‹åŒ–æ£€æµ‹ç³»ç»Ÿï¼ˆç‰©ç†å‘¨æœŸ=24å°æ—¶ï¼Œé‡‡æ ·é—´éš”=5åˆ†é’Ÿï¼‰
    detector = IndustrialAnomalyDetector(
        physical_cycle=86400,  # 24å°æ—¶ç‰©ç†å‘¨æœŸï¼ˆç§’ï¼‰
        sampling_interval=300   # 5åˆ†é’Ÿé‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
    )
    
    # 3. æ¨¡æ‹Ÿæ•°æ®æµå¤„ç†
    all_data = generate_industrial_data()
    results = []
    
    # åˆ†æ‰¹å¤„ç†æ¨¡æ‹Ÿå®æ—¶æµ
    batch_size = 12  # æ¯å°æ—¶å¤„ç†ä¸€æ¬¡ï¼ˆ12ä¸ª5åˆ†é’Ÿç‚¹ï¼‰
    for i in range(0, len(all_data), batch_size):
        batch = all_data.iloc[i:i+batch_size]
        
        print(f"\n=== å¤„ç†æ‰¹æ¬¡ {i//batch_size+1}/{(len(all_data)//batch_size)} ===")
        print(f"æ—¶é—´èŒƒå›´: {batch['timestamp'].min()} è‡³ {batch['timestamp'].max()}")
        
        # æ ¸å¿ƒæ£€æµ‹è°ƒç”¨
        scores, labels = detector.process_stream(batch)
        
        # å­˜å‚¨ç»“æœ
        batch_results = batch.copy()
        batch_results['anomaly_score'] = scores
        batch_results['is_anomaly'] = labels
        results.append(batch_results)
        
        # æ‰“å°å¼‚å¸¸æ£€æµ‹ç»“æœ
        if np.any(labels):
            print(f"ğŸš© æ£€æµ‹åˆ°å¼‚å¸¸: {np.sum(labels)}ä¸ª")
            anomalies = batch_results[labels]
            print(anomalies[['timestamp', 'value', 'anomaly_score']])
    
    # 4. ç»“æœåˆ†æ
    final_results = pd.concat(results)
    anomalies = final_results[final_results['is_anomaly']]
    
    print(f"\nğŸ” æ£€æµ‹å®Œæˆ! å…±å‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸ç‚¹")
    print("æœ€è¿‘5ä¸ªå¼‚å¸¸ç‚¹:")
    print(anomalies[['timestamp', 'value', 'anomaly_score']].tail(5))
    
    # 5. ç³»ç»ŸçŠ¶æ€ä¿å­˜
    detector.save_state('industrial_detector_state.pkl')